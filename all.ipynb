{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce253a-b759-4206-bb1c-5835bf8cee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep Learning Code\n",
    "#Importing related packages\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import glob\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import timm\n",
    "\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms, utils\n",
    "from tqdm.notebook import tqdm\n",
    "print(f\"Torch: {torch.__version__}\")\n",
    "\n",
    "import gc\n",
    "import random\n",
    "from PIL import Image,ImageFilter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "#Empty the GPU\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "#Add pretzel noise\n",
    "class AddSaltPepperNoise(object):\n",
    "\n",
    "    def __init__(self, density=0,p=0.5):\n",
    "        self.density = density\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.uniform(0, 1) < self.p:  # probabilistic judgement\n",
    "            img = np.array(img)  # Image to numpy\n",
    "            h, w, c = img.shape\n",
    "            Nd = self.density\n",
    "            Sd = 1 - Nd\n",
    "            mask = np.random.choice((0, 1, 2), size=(h, w, 1), p=[Nd / 2.0, Nd / 2.0, Sd])  # Generate a mask for a channel\n",
    "            mask = np.repeat(mask, c, axis=2)  # Duplicate in the dimension of the channel to generate a coloured mask\n",
    "            img[mask == 0] = 0  \n",
    "            img[mask == 1] = 255  \n",
    "            img = Image.fromarray(img.astype('uint8')).convert('RGB')  # numpy to image\n",
    "            return img\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "#Adding Gaussian Noise\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0.0, variance=1.0, amplitude=1.0):\n",
    "\n",
    "        self.mean = mean\n",
    "        self.variance = variance\n",
    "        self.amplitude = amplitude\n",
    "\n",
    "    def __call__(self, img):\n",
    "\n",
    "        img = np.array(img)\n",
    "        h, w, c = img.shape\n",
    "        N = self.amplitude * np.random.normal(loc=self.mean, scale=self.variance, size=(h, w, 1))\n",
    "        N = np.repeat(N, c, axis=2)\n",
    "        img = N + img\n",
    "        img[img > 255] = 255                       \n",
    "        img = Image.fromarray(img.astype('uint8')).convert('RGB')\n",
    "        return img\n",
    "\n",
    "#Add Blur\n",
    "class Addblur(object):\n",
    "\n",
    "    def __init__(self, p=0.5,blur=\"normal\"):\n",
    "        # self.density = density\n",
    "        self.p = p\n",
    "        self.blur= blur\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.uniform(0, 1) < self.p:  # 概率的判断\n",
    "       \t\t #standard ambiguity\n",
    "            if self.blur== \"normal\":\n",
    "                img = img.filter(ImageFilter.BLUR)\n",
    "                return img\n",
    "            #Gaussian blur\n",
    "            if self.blur== \"Gaussian\":\n",
    "                img = img.filter(ImageFilter.GaussianBlur)\n",
    "                return img\n",
    "            #mean value ambiguity\n",
    "            if self.blur== \"mean\":\n",
    "                img = img.filter(ImageFilter.BoxBlur)\n",
    "                return img\n",
    "\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "#Append data from each epoch to an Excel file\n",
    "def append_epoch_data_to_excel(epoch, train_loss, train_acc, val_loss, val_acc, excel_path):\n",
    "    epoch_data = pd.DataFrame({\n",
    "        'Epoch': [epoch],\n",
    "        'Train Loss': [train_loss],\n",
    "        'Train Acc': [train_acc],\n",
    "        'Val Loss': [val_loss],\n",
    "        'Val Acc': [val_acc]\n",
    "    })\n",
    "    try:\n",
    "        with pd.ExcelWriter(excel_path, mode='a', engine='openpyxl', if_sheet_exists='overlay') as writer:\n",
    "            epoch_data.to_excel(writer, index=False, header=False, startrow=writer.sheets['Sheet1'].max_row)\n",
    "    except FileNotFoundError:\n",
    "        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "            epoch_data.to_excel(writer, index=False)\n",
    "            \n",
    "#Data Enhancement:\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomOrder([  # random order\n",
    "        # random rotation\n",
    "        transforms.RandomApply([transforms.RandomRotation([-30, 30])], p=0.3),\n",
    "        # Horizontal Flip\n",
    "        transforms.RandomHorizontalFlip(p=0.3),\n",
    "        # Random cropping and scaling\n",
    "        transforms.RandomApply([transforms.RandomResizedCrop((150, 200), scale=(0.4, 0.8), ratio=(1./5., 5./1.))], p=0.5),\n",
    "        # Randomly changing image properties: brightness, contrast, saturation and hue\n",
    "        transforms.RandomApply([transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)], p=0.5),\n",
    "        # random blur\n",
    "        transforms.RandomApply([transforms.Compose([Addblur(p=1, blur=\"Gaussian\")])], p=0.5),\n",
    "        # Randomly added pretzel noise\n",
    "        transforms.RandomApply([transforms.Compose([AddSaltPepperNoise(0.05, 1)])], p=0.5),\n",
    "        # Add random Gaussian noise\n",
    "        transforms.RandomApply([transforms.Compose([AddGaussianNoise(mean=random.uniform(0.5, 1.5), variance=0.5, amplitude=random.uniform(0, 45))])], p=0.5),\n",
    "    ]),\n",
    "    transforms.Resize((224, 224)),  \n",
    "    # Converting images to PyTorch tensors\n",
    "    transforms.ToTensor(),\n",
    "    # Normalisation based on ImageNet mean and standard deviation\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Training settings\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "lr = 2e-5\n",
    "gamma = 0.7\n",
    "seed = 42\n",
    "\n",
    "train_dir = 'train'\n",
    "val_dir = 'validation'\n",
    "test_dir = 'test'\n",
    "\n",
    "#Load Dataset\n",
    "train_img_data = torchvision.datasets.ImageFolder(train_dir,\n",
    "                                            transform=train_transforms,\n",
    "                                            )\n",
    "\n",
    "val_img_data = torchvision.datasets.ImageFolder(val_dir,\n",
    "                                            transform=val_transforms,\n",
    "                                            )\n",
    "test_img_data = torchvision.datasets.ImageFolder(test_dir,\n",
    "                                            transform=test_transforms,\n",
    "                                            )\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_img_data, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "val_data_loader = torch.utils.data.DataLoader(val_img_data, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_img_data, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "print(len(train_data_loader.dataset))\n",
    "print(len(val_data_loader.dataset))\n",
    "print(len(test_data_loader.dataset))\n",
    "\n",
    "# Get a list of all model names\n",
    "model_list = timm.list_models()\n",
    "\n",
    "#Getting the model\n",
    "model = timm.create_model('swin_small_patch4_window7_224', pretrained=False, num_classes=50).to(device)\n",
    "          \n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer \n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=25, eta_min=1e-6)\n",
    "\n",
    "#Training Models\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "\n",
    "    #for data, label in tqdm(train_loader):\n",
    "    for data, label in tqdm(train_data_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = (output.argmax(dim=1) == label).float().mean()\n",
    "        epoch_accuracy += acc / len(train_data_loader)\n",
    "        epoch_loss += loss / len(train_data_loader)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        epoch_val_accuracy = 0\n",
    "        epoch_val_loss = 0\n",
    "        for data, label in val_data_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            val_output = model(data)\n",
    "            val_loss = criterion(val_output, label)\n",
    "\n",
    "            acc = (val_output.argmax(dim=1) == label).float().mean()\n",
    "            epoch_val_accuracy += acc / len(val_data_loader)\n",
    "            epoch_val_loss += val_loss / len(val_data_loader)\n",
    "            \n",
    "    \n",
    "    excel_path = \"swin_small_patch4_window7_224.xlsx\"  \n",
    "    append_epoch_data_to_excel(\n",
    "    epoch, \n",
    "    epoch_loss.item(),  \n",
    "    epoch_accuracy.item(), \n",
    "    epoch_val_loss.item(), \n",
    "    epoch_val_accuracy.item(), \n",
    "    excel_path)\n",
    "    print(\n",
    "            f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
    "        )\n",
    "\n",
    "    torch.save(model, './swin_small_patch4_window7_224.pt')  # Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56033d8-9f17-42f2-a297-e68b555e1041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#超参数优化\n",
    "!pip install --upgrade typing_extensions\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import glob\n",
    "from itertools import chain\n",
    "import random\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset as torch_Dataset  # Renamed to avoid conflict with data module\n",
    "from torchvision import datasets, transforms, utils\n",
    "from PIL import Image, ImageFilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import timm\n",
    "import json\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import torchvision\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score  # Combined import for sklearn metrics\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "#Empty the GPU\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "#Add pretzel noise\n",
    "class AddSaltPepperNoise(object):\n",
    "\n",
    "    def __init__(self, density=0,p=0.5):\n",
    "        self.density = density\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.uniform(0, 1) < self.p:  # probabilistic judgement\n",
    "            img = np.array(img)  # Image to numpy\n",
    "            h, w, c = img.shape\n",
    "            Nd = self.density\n",
    "            Sd = 1 - Nd\n",
    "            mask = np.random.choice((0, 1, 2), size=(h, w, 1), p=[Nd / 2.0, Nd / 2.0, Sd])  # Generate a mask for a channel\n",
    "            mask = np.repeat(mask, c, axis=2)  # Duplicate in the dimension of the channel to generate a coloured mask\n",
    "            img[mask == 0] = 0  \n",
    "            img[mask == 1] = 255  \n",
    "            img = Image.fromarray(img.astype('uint8')).convert('RGB')  # numpy to image\n",
    "            return img\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "#Adding Gaussian Noise\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0.0, variance=1.0, amplitude=1.0):\n",
    "\n",
    "        self.mean = mean\n",
    "        self.variance = variance\n",
    "        self.amplitude = amplitude\n",
    "\n",
    "    def __call__(self, img):\n",
    "\n",
    "        img = np.array(img)\n",
    "        h, w, c = img.shape\n",
    "        N = self.amplitude * np.random.normal(loc=self.mean, scale=self.variance, size=(h, w, 1))\n",
    "        N = np.repeat(N, c, axis=2)\n",
    "        img = N + img\n",
    "        img[img > 255] = 255                       \n",
    "        img = Image.fromarray(img.astype('uint8')).convert('RGB')\n",
    "        return img\n",
    "\n",
    "#Add Blur\n",
    "class Addblur(object):\n",
    "\n",
    "    def __init__(self, p=0.5,blur=\"normal\"):\n",
    "        # self.density = density\n",
    "        self.p = p\n",
    "        self.blur= blur\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.uniform(0, 1) < self.p:  # 概率的判断\n",
    "       \t\t #standard ambiguity\n",
    "            if self.blur== \"normal\":\n",
    "                img = img.filter(ImageFilter.BLUR)\n",
    "                return img\n",
    "            #Gaussian blur\n",
    "            if self.blur== \"Gaussian\":\n",
    "                img = img.filter(ImageFilter.GaussianBlur)\n",
    "                return img\n",
    "            #mean value ambiguity\n",
    "            if self.blur== \"mean\":\n",
    "                img = img.filter(ImageFilter.BoxBlur)\n",
    "                return img\n",
    "\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "#Append data from each epoch to an Excel file\n",
    "def append_epoch_data_to_excel(epoch, train_loss, train_acc, val_loss, val_acc, excel_path):\n",
    "    epoch_data = pd.DataFrame({\n",
    "        'Epoch': [epoch],\n",
    "        'Train Loss': [train_loss],\n",
    "        'Train Acc': [train_acc],\n",
    "        'Val Loss': [val_loss],\n",
    "        'Val Acc': [val_acc]\n",
    "    })\n",
    "    try:\n",
    "        with pd.ExcelWriter(excel_path, mode='a', engine='openpyxl', if_sheet_exists='overlay') as writer:\n",
    "            epoch_data.to_excel(writer, index=False, header=False, startrow=writer.sheets['Sheet1'].max_row)\n",
    "    except FileNotFoundError:\n",
    "        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "            epoch_data.to_excel(writer, index=False)\n",
    "            \n",
    "# Data enhancement parameters adjusted by Optuna            \n",
    "def optimize_hyperparameters(trial):\n",
    "    lr = trial.suggest_uniform('lr', 0, 1e-4)\n",
    "    bn = trial.suggest_int('batchsize', 8, 52)\n",
    "    T_max=trial.suggest_int('T_max', 5, 25)\n",
    "    weight_decay=trial.suggest_uniform('weight_decay', 1e-3, 1e-1)\n",
    "    p_RandomRotation = trial.suggest_float('p_RandomRotation', 0.1, 1)\n",
    "    p_RandomHorizontalFlip = trial.suggest_float('p_RandomHorizontalFlip', 0.1, 1)\n",
    "    crop_size_h = trial.suggest_int('crop_size_h', 150, 200)  \n",
    "    crop_size_w = trial.suggest_int('crop_size_w', 150, 200)  \n",
    "    p_RandomResizedCrop = trial.suggest_float('p_RandomResizedCrop', 0.1, 1)\n",
    "    brightness = trial.suggest_float('brightness', 0.1, 0.5)\n",
    "    contrast = trial.suggest_float('contrast', 0.1, 0.5)\n",
    "    saturation = trial.suggest_float('saturation', 0.1, 0.5)\n",
    "    hue = trial.suggest_float('hue', 0.1, 0.5)\n",
    "    p_ColorJitter = trial.suggest_float('p_ColorJitter', 0.1, 1)  \n",
    "    p_Addblur = trial.suggest_float('p_Addblur', 0.1, 1)\n",
    "    p_AddSaltPepperNoise = trial.suggest_float('p_AddSaltPepperNoise', 0.1, 1)\n",
    "    mean_GaussianNoise = trial.suggest_float('mean_GaussianNoise', 0.1, 1)\n",
    "    variance_GaussianNoise = trial.suggest_float('variance_GaussianNoise', 0.1, 0.9)\n",
    "    amplitude_GaussianNoise = trial.suggest_float('amplitude_GaussianNoise', 0.1, 1)\n",
    "    p_AddGaussianNoise = trial.suggest_float('p_AddGaussianNoise', 0.1, 1)  \n",
    "    \n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomOrder([\n",
    "            transforms.RandomApply([transforms.RandomRotation([-30, 30])], p=p_RandomRotation),\n",
    "            transforms.RandomHorizontalFlip(p=p_RandomHorizontalFlip),\n",
    "            transforms.RandomApply([transforms.RandomResizedCrop((crop_size_h, crop_size_w), scale=(0.5, 1.0), ratio=(1./5., 5./1.))], p=p_RandomResizedCrop),\n",
    "            transforms.RandomApply([transforms.ColorJitter(brightness=brightness, contrast=contrast, saturation=saturation, hue=hue)], p=p_ColorJitter),\n",
    "            transforms.RandomApply([Addblur(p=p_Addblur, blur=\"Gaussian\")], p=p_Addblur),\n",
    "            transforms.RandomApply([AddSaltPepperNoise(density=0.05, p=1)], p=p_AddSaltPepperNoise),\n",
    "            transforms.RandomApply([AddGaussianNoise(mean=mean_GaussianNoise, variance=variance_GaussianNoise, amplitude=amplitude_GaussianNoise)], p=p_AddGaussianNoise),\n",
    "        ]),\n",
    "        transforms.Resize((224, 224)),  \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "#Importing data sets\n",
    "    train_dir = 'train'\n",
    "    val_dir = 'validation'\n",
    "    test_dir = 'test'\n",
    "\n",
    "    train_img_data = torchvision.datasets.ImageFolder(train_dir,\n",
    "                                                transform=train_transforms,\n",
    "                                                )\n",
    "\n",
    "    val_img_data = torchvision.datasets.ImageFolder(val_dir,\n",
    "                                                transform=val_transforms,\n",
    "                                                )\n",
    "    test_img_data = torchvision.datasets.ImageFolder(test_dir,\n",
    "                                                transform=test_transforms,\n",
    "                                                )\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_img_data, batch_size=bn, shuffle=True, num_workers=8)\n",
    "    val_data_loader = torch.utils.data.DataLoader(val_img_data, batch_size=bn, shuffle=True, num_workers=8)\n",
    "    test_data_loader = torch.utils.data.DataLoader(test_img_data, batch_size=bn, shuffle=True, num_workers=8)\n",
    "\n",
    "    print(len(train_data_loader.dataset))\n",
    "    print(len(val_data_loader.dataset))\n",
    "    print(len(test_data_loader.dataset))\n",
    "\n",
    "    model = timm.create_model(\"swin_small_patch4_window7_224\", pretrained=False, num_classes=50).to(device)\n",
    "   \n",
    "    # loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=1e-6)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "\n",
    "        #for data, label in tqdm(train_loader):\n",
    "        for data, label in tqdm(train_data_loader):\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc = (output.argmax(dim=1) == label).float().mean()\n",
    "            epoch_accuracy += acc / len(train_data_loader)\n",
    "            epoch_loss += loss / len(train_data_loader)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            epoch_val_accuracy = 0\n",
    "            epoch_val_loss = 0\n",
    "            for data, label in val_data_loader:\n",
    "                data = data.to(device)\n",
    "                label = label.to(device)\n",
    "\n",
    "                val_output = model(data)\n",
    "                val_loss = criterion(val_output, label)\n",
    "\n",
    "                acc = (val_output.argmax(dim=1) == label).float().mean()\n",
    "                epoch_val_accuracy += acc / len(val_data_loader)\n",
    "                epoch_val_loss += val_loss / len(val_data_loader)\n",
    "\n",
    "        \n",
    "        excel_path = \"training_data_swin_small_patch4_window7_224_5.xlsx\"  \n",
    "        append_epoch_data_to_excel(\n",
    "    epoch, \n",
    "    epoch_loss.item(),  \n",
    "    epoch_accuracy.item(), \n",
    "    epoch_val_loss.item(), \n",
    "    epoch_val_accuracy.item(), \n",
    "    excel_path)\n",
    "        print(\n",
    "            f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
    "        )\n",
    "\n",
    "    model_save_path = f\"swin_small_patch4_window7_224_trial_{trial.number}_epoch_{epoch+1}.pt\"\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "    # return the validation accuracy as the objective value\n",
    "    return epoch_val_loss\n",
    "\n",
    "#Preservation of experimental results\n",
    "def save_trial_to_excel(study, trial):\n",
    "    trial_data = trial.params\n",
    "    trial_data['trial_number'] = trial.number\n",
    "    trial_data['epoch_val_loss'] = trial.value\n",
    "    df_trial = pd.DataFrame([trial_data])\n",
    "    \n",
    "    excel_path = \"optuna_trials_swin_small_patch4_window7_224_5.xlsx\"\n",
    "\n",
    "    try:\n",
    "        with pd.ExcelWriter(excel_path, mode='a', engine='openpyxl', if_sheet_exists='overlay') as writer:\n",
    "            df_trial.to_excel(writer, index=False, header=False, startrow=writer.sheets['Sheet1'].max_row)\n",
    "    except FileNotFoundError:\n",
    "        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "            df_trial.to_excel(writer, index=False)\n",
    "\n",
    "#Defining parameters\n",
    "epochs=50\n",
    "torch.manual_seed(42)            \n",
    "\n",
    "#Perform a hyperparametric search\n",
    "study = optuna.create_study()\n",
    "study.optimize(lambda trial: optimize_hyperparameters(trial), n_trials=50, callbacks=[save_trial_to_excel])\n",
    "\n",
    "# Save the results of the optimisation test to another Excel file.\n",
    "best_trial = study.best_trial\n",
    "best_trial_data = best_trial.params\n",
    "best_trial_data['trial_number'] = best_trial.number\n",
    "best_trial_data['epoch_val_loss'] = best_trial.value\n",
    "df_best_trial = pd.DataFrame([best_trial_data])\n",
    "\n",
    "excel_path_best_trial = \"optuna_best_trial_swin_small_patch4_window7_224.xlsx\"\n",
    "df_best_trial.to_excel(excel_path_best_trial, index=False)\n",
    "\n",
    "print(f\"All trials are saved to optuna_trials.xlsx after each trial.\")\n",
    "print(f\"Best trial is saved to optuna_best_trial.xlsx.\")\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"epoch_val_loss: \", trial.value)\n",
    "print(\"Hyperparameters: \", trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539d6451-3e5f-4e0d-857e-90c055bafa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Evaluation Code\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "#Loading Models\n",
    "model_path = 'swin_small_patch4_window7_224.pt'\n",
    "model = torch.load(model_path, map_location=device)\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.eval() \n",
    "prob_all = []\n",
    "lable_all = []\n",
    "prob_score=[]\n",
    "i=0\n",
    "for data, label in tqdm(test_data_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        l2 = label.cpu().detach().numpy()\n",
    "        lable_all.extend(l2)\n",
    "        prob = model(data) \n",
    "        prob = prob.cpu().detach().numpy() \n",
    "        prob_score.extend(prob) \n",
    "        prob_all.extend(np.argmax(prob,axis=1))\n",
    "        i=i+1\n",
    "print(i)\n",
    "\n",
    "root_dir = \"test\"\n",
    "\n",
    "#Importing dataset labels\n",
    "try:\n",
    "    target_names = sorted([name for name in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, name))])\n",
    "    print(\"Sequentially imported dataset labels (folders):\", target_names)\n",
    "except Exception as e:\n",
    "    print(\"Error importing dataset labels:\", str(e))\n",
    "\n",
    "print(\"Accuracy:{:.4f}\".format(accuracy_score(lable_all, prob_all) ))\n",
    "print(\"Recall:{:.4f}\".format(recall_score(lable_all, prob_all,average='macro') ))\n",
    "print(\"Precision:{:.4f}\".format(precision_score(lable_all, prob_all,average='macro') ))\n",
    "print(\"f1_score:{:.4f}\".format(f1_score(lable_all, prob_all,average='macro') ))\n",
    "\n",
    "print(classification_report(lable_all,prob_all,target_names=target_names,digits=4))\n",
    "print(confusion_matrix(prob_all,lable_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d37f8-2891-4a4c-9ea6-821600e0964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Obtain the confusion matrix\n",
    "conf_mat=confusion_matrix(prob_all,lable_all)\n",
    "root_dir='test'\n",
    "\n",
    "#Importing dataset categories\n",
    "try:\n",
    "    class_label = sorted([name for name in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, name))])\n",
    "    print(\"Sequentially imported dataset labels (folders):\", class_label)\n",
    "except Exception as e:\n",
    "    print(\"Error importing dataset labels:\", str(e))\n",
    "    \n",
    "df_cm = pd.DataFrame(conf_mat, index=class_label, columns=class_label)\n",
    "df_cm\n",
    "\n",
    "#Mapping the confusion matrix\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "heatmap=sns.heatmap(df_cm, annot=True, fmt=\"d\",cmap=\"YlGnBu\")\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(),rotation=0, ha='right')\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(),rotation=45, ha='right')\n",
    "plt.title('Confusion Matrix for Test Set Using Swin transformer Model')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.savefig(\"i_swin_small_patch4_window7_224\", bbox_inches='tight', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a81be1-600c-418f-8d14-2ec4e904ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.data import DataLoader, sampler, random_split\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "root_dir = \"test\"\n",
    "\n",
    "# Importing dataset labels\n",
    "try:\n",
    "    target_names = sorted([name for name in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, name))])\n",
    "    print(\"Sequentially imported dataset labels (folders):\", target_names)\n",
    "except Exception as e:\n",
    "    print(\"Error importing dataset labels:\", str(e))\n",
    "\n",
    "# Loading Models\n",
    "model_path = 'swin_small_patch4_window7_224.pt'\n",
    "model = torch.load(model_path, map_location=device)\n",
    "\n",
    "# Get the predicted values of the training set\n",
    "def get_representations(model, iterator):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    outputs = []\n",
    "    intermediates = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, label in tqdm(iterator):\n",
    "            data = data.to(device)\n",
    "            #label = label.to(device)\n",
    "            l2 = label.cpu().detach().numpy()\n",
    "            lable_all.extend(l2)\n",
    "            prob = model(data) \n",
    "            outputs.append(prob.cpu())\n",
    "            labels.append(label)\n",
    "        \n",
    "\n",
    "    outputs = torch.cat(outputs, dim = 0)\n",
    "    labels = torch.cat(labels, dim = 0)\n",
    "\n",
    "    return outputs, labels\n",
    "\n",
    "outputs, labels = get_representations(model, test_data_loader)\n",
    "\n",
    "# t-SNE\n",
    "def get_tsne(data, n_components = 2, n_images = None):\n",
    "    \n",
    "    if n_images is not None:\n",
    "        data = data[:n_images]\n",
    "        \n",
    "    tsne = manifold.TSNE(n_components = n_components, early_exaggeration=3, random_state = 8, perplexity=70,learning_rate=300)\n",
    "    tsne_data = tsne.fit_transform(data)\n",
    "    return tsne_data\n",
    "\n",
    "# Mapping t-SNE\n",
    "def plot_representations(data, labels, classes, n_images=None):\n",
    "    if n_images is not None:\n",
    "        data = data[:n_images]\n",
    "        labels = labels[:n_images]\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 10))  \n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    # Create enough colours to match the number of categories\n",
    "    cmap = plt.get_cmap('nipy_spectral')\n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, len(classes))]\n",
    "\n",
    "    # Plotting a scatterplot for each category\n",
    "    for i, class_name in enumerate(classes):\n",
    "        idx = labels == i  \n",
    "        ax.scatter(data[idx, 0], data[idx, 1], label=class_name, color=colors[i])\n",
    "\n",
    "    plt.title('VGG11 Model t-SNE Scatter Plot on Test Set')\n",
    "    plt.xlabel('t-SNE Dim 1')\n",
    "    plt.ylabel('t-SNE Dim 2')\n",
    "    # Place the legend on the right side outside the graphic\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize='x-small', markerscale=2)\n",
    "    # Resize the main image to make room for the legend\n",
    "    plt.tight_layout(rect=[0, 0, 0.75, 1])  \n",
    "    plt.savefig(\"i_t-SNE_VGG11_newdata_2\", bbox_inches='tight', dpi=500)\n",
    "    plt.show()\n",
    "    \n",
    "N_IMAGES = 6_000    \n",
    "output_tsne_data = get_tsne(outputs)\n",
    "plot_representations(output_tsne_data, labels,target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f0d91f-286e-4d5e-b499-b1b8e605fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DER+LwF code\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim import SGD\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import timm\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Importing the necessary components of the avalanche library\n",
    "from avalanche.benchmarks.generators import nc_benchmark\n",
    "from avalanche.training import DER\n",
    "from avalanche.evaluation.metrics import accuracy_metrics, loss_metrics,bwt_metrics,forgetting_metrics,forward_transfer_metrics\n",
    "from avalanche.logging import InteractiveLogger, TensorboardLogger,CSVLogger,TextLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training.plugins import LwFPlugin\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()  \n",
    "\n",
    "# Setting parameters\n",
    "lr = 0.0000248597317274012\n",
    "batch_size = 37\n",
    "\n",
    "#Add pretzel noise\n",
    "class AddSaltPepperNoise(object):\n",
    "\n",
    "    def __init__(self, density=0,p=0.5):\n",
    "        self.density = density\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.uniform(0, 1) < self.p:  # probabilistic judgement\n",
    "            img = np.array(img)  # Image to numpy\n",
    "            h, w, c = img.shape\n",
    "            Nd = self.density\n",
    "            Sd = 1 - Nd\n",
    "            mask = np.random.choice((0, 1, 2), size=(h, w, 1), p=[Nd / 2.0, Nd / 2.0, Sd])  # Generate a mask for a channel\n",
    "            mask = np.repeat(mask, c, axis=2)  # Duplicate in the dimension of the channel to generate a coloured mask\n",
    "            img[mask == 0] = 0  \n",
    "            img[mask == 1] = 255  \n",
    "            img = Image.fromarray(img.astype('uint8')).convert('RGB')  # numpy to image\n",
    "            return img\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "#Adding Gaussian Noise\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0.0, variance=1.0, amplitude=1.0):\n",
    "\n",
    "        self.mean = mean\n",
    "        self.variance = variance\n",
    "        self.amplitude = amplitude\n",
    "\n",
    "    def __call__(self, img):\n",
    "\n",
    "        img = np.array(img)\n",
    "        h, w, c = img.shape\n",
    "        N = self.amplitude * np.random.normal(loc=self.mean, scale=self.variance, size=(h, w, 1))\n",
    "        N = np.repeat(N, c, axis=2)\n",
    "        img = N + img\n",
    "        img[img > 255] = 255                       \n",
    "        img = Image.fromarray(img.astype('uint8')).convert('RGB')\n",
    "        return img\n",
    "\n",
    "#Add Blur\n",
    "class Addblur(object):\n",
    "\n",
    "    def __init__(self, p=0.5,blur=\"normal\"):\n",
    "        # self.density = density\n",
    "        self.p = p\n",
    "        self.blur= blur\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.uniform(0, 1) < self.p:  # 概率的判断\n",
    "       \t\t #standard ambiguity\n",
    "            if self.blur== \"normal\":\n",
    "                img = img.filter(ImageFilter.BLUR)\n",
    "                return img\n",
    "            #Gaussian blur\n",
    "            if self.blur== \"Gaussian\":\n",
    "                img = img.filter(ImageFilter.GaussianBlur)\n",
    "                return img\n",
    "            #mean value ambiguity\n",
    "            if self.blur== \"mean\":\n",
    "                img = img.filter(ImageFilter.BoxBlur)\n",
    "                return img\n",
    "\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "# Define a function for saving results in real time\n",
    "def save_results_to_file(results, base_filename='results'):\n",
    "    # Get the current date and time, formatted as a string\n",
    "    current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    filename = f'{base_filename}_{current_time}.txt'  # 构造文件名\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "        \n",
    "# Image preprocessing for training and validation\n",
    "train_transforms = transforms.Compose([\n",
    "   transforms.RandomOrder([  \n",
    "        transforms.RandomApply([transforms.RandomRotation([-30, 30])], p=0.763308964823317),\n",
    "        transforms.RandomHorizontalFlip(p=0.276583598372167),\n",
    "        transforms.RandomApply([transforms.RandomResizedCrop((163, 162), scale=(0.4, 0.8), ratio=(1./5., 5./1.))], p=0.298899012089413),\n",
    "        transforms.RandomApply([transforms.ColorJitter(brightness=0.385039910829659, contrast=0.469248558722806, saturation=0.222816942158365, hue=0.332066381812662)], p=0.577596666960827),\n",
    "        transforms.RandomApply([transforms.Compose([Addblur(p=1, blur=\"Gaussian\")])], p=0.321736665705054),\n",
    "        transforms.RandomApply([transforms.Compose([AddSaltPepperNoise(0.05, 1)])], p=0.155074274460154),\n",
    "        transforms.RandomApply([transforms.Compose([AddGaussianNoise(mean=0.432936311664746, variance=0.142237563518334, amplitude=0.34620281978491)])], p=0.489988627632719),\n",
    "    ]),\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dir = 'train'\n",
    "test_dir = 'test'\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=train_dir, transform=train_transforms)\n",
    "test_dataset = torchvision.datasets.ImageFolder(root=test_dir, transform=test_transforms)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "print(f\"train dataset size: {len(train_data_loader.dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_data_loader.dataset)}\")\n",
    "\n",
    "# Create class incremental learning scenarios\n",
    "scenario = nc_benchmark(\n",
    "    train_dataset=train_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    n_experiences=5,  \n",
    "    task_labels=False,\n",
    "    shuffle=True,\n",
    "    seed=1234,\n",
    ")\n",
    "\n",
    "#Setting the path to save the file\n",
    "tensorboard_log_dir = \"path_to_your_tensorboard_log_dir_swin\" \n",
    "csv_log_folder = \"path_to_your_csv_log_folder_swin\"  \n",
    "\n",
    "# Setting the log file path\n",
    "text_log_file_path = \"path_to_your_text_log_file_swin.txt\"\n",
    "\n",
    "# Create an instance of TextLogger, specifying the output file\n",
    "text_logger = TextLogger(file=open(text_log_file_path, \"w\"))\n",
    "\n",
    "# Define evaluation plug-ins\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(experience=True, stream=True, trained_experience=True),\n",
    "    loss_metrics(epoch=True, experience=True, stream=True),\n",
    "    bwt_metrics(experience=True, stream=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    loggers=[InteractiveLogger(), TensorboardLogger(tensorboard_log_dir),CSVLogger(csv_log_folder),text_logger]\n",
    ")\n",
    "\n",
    "lwf_plugin = LwFPlugin(alpha=[0, 1 / 2, 2 * (2 / 3), 3 * (3 / 4), 4 * (4 / 5)], temperature=2)\n",
    "\n",
    "# Loading Models\n",
    "model = timm.create_model('swin_small_patch4_window7_224', pretrained=False, num_classes=50)\n",
    "model.to('cuda')\n",
    "\n",
    "# Configuration Optimiser\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "\n",
    "# Initialising a policy instance\n",
    "strategy = DER(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    mem_size = 200, \n",
    "    batch_size_mem = 10, \n",
    "    alpha = 0.1, \n",
    "    beta = 0.5, \n",
    "    train_mb_size=64,\n",
    "    train_epochs=50,\n",
    "    eval_mb_size=100,\n",
    "    device='cuda',\n",
    "    plugins=[lwf_plugin],\n",
    "    # Pass in the eval_plugin instance\n",
    "    evaluator=eval_plugin,  \n",
    ")\n",
    "    \n",
    "print(\"Starting experiment...\")\n",
    "results = []\n",
    "for experience in scenario.train_stream:\n",
    "    print(\"Start of experience \", experience.current_experience)\n",
    "    strategy.train(experience, num_workers=4)\n",
    "    print(\"Training completed\")\n",
    "\n",
    "    print(\"Computing accuracy on the whole test set\")\n",
    "    # Save results after each assessment\n",
    "    current_result = strategy.eval(scenario.test_stream, num_workers=8)\n",
    "    results.append(current_result)\n",
    "    # Calling a function to save results in real time\n",
    "    save_results_to_file(results, 'DER_lwf_realtime_results_swin')  \n",
    "\n",
    "print(f\"Test metrics:\\n{results}\")\n",
    "\n",
    "torch.save(model, './swin_DER_lwf.pt')\n",
    "\n",
    "# Save results to file\n",
    "results_str = json.dumps(results, indent=4)\n",
    "\n",
    "# Eventually save again to make sure the final result is also saved\n",
    "save_results_to_file(results, 'DER_lwf_final_results_swin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
